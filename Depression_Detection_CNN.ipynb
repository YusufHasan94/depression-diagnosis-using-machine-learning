{"cells":[{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Scno1HLmbmK-","executionInfo":{"status":"ok","timestamp":1704821512512,"user_tz":-360,"elapsed":47755,"user":{"displayName":"usecase 11a","userId":"10388319228409710122"}},"outputId":"a8727064-5b4b-4748-cb7c-250449661e01"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/utils/extmath.py:1047: RuntimeWarning: invalid value encountered in divide\n","  updated_mean = (last_sum + new_sum) / updated_sample_count\n","/usr/local/lib/python3.10/dist-packages/sklearn/utils/extmath.py:1052: RuntimeWarning: invalid value encountered in divide\n","  T = new_sum / new_sample_count\n","/usr/local/lib/python3.10/dist-packages/sklearn/utils/extmath.py:1072: RuntimeWarning: invalid value encountered in divide\n","  new_unnormalized_variance -= correction**2 / new_sample_count\n"]},{"output_type":"stream","name":"stdout","text":["Model: \"sequential_5\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv1d_15 (Conv1D)          (None, 1147, 32)          128       \n","                                                                 \n"," max_pooling1d_15 (MaxPooli  (None, 573, 32)           0         \n"," ng1D)                                                           \n","                                                                 \n"," conv1d_16 (Conv1D)          (None, 571, 64)           6208      \n","                                                                 \n"," max_pooling1d_16 (MaxPooli  (None, 285, 64)           0         \n"," ng1D)                                                           \n","                                                                 \n"," conv1d_17 (Conv1D)          (None, 283, 128)          24704     \n","                                                                 \n"," max_pooling1d_17 (MaxPooli  (None, 141, 128)          0         \n"," ng1D)                                                           \n","                                                                 \n"," flatten_5 (Flatten)         (None, 18048)             0         \n","                                                                 \n"," dense_10 (Dense)            (None, 64)                1155136   \n","                                                                 \n"," dense_11 (Dense)            (None, 1)                 65        \n","                                                                 \n","=================================================================\n","Total params: 1186241 (4.53 MB)\n","Trainable params: 1186241 (4.53 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","Epoch 1/20\n","22/22 [==============================] - 3s 91ms/step - loss: nan - accuracy: 0.7882 - val_loss: nan - val_accuracy: 0.8553\n","Epoch 2/20\n","22/22 [==============================] - 2s 81ms/step - loss: nan - accuracy: 0.7882 - val_loss: nan - val_accuracy: 0.8553\n","Epoch 3/20\n","22/22 [==============================] - 2s 77ms/step - loss: nan - accuracy: 0.7882 - val_loss: nan - val_accuracy: 0.8553\n","Epoch 4/20\n","22/22 [==============================] - 2s 78ms/step - loss: nan - accuracy: 0.7882 - val_loss: nan - val_accuracy: 0.8553\n","Epoch 5/20\n","22/22 [==============================] - 3s 117ms/step - loss: nan - accuracy: 0.7882 - val_loss: nan - val_accuracy: 0.8553\n","Epoch 6/20\n","22/22 [==============================] - 3s 138ms/step - loss: nan - accuracy: 0.7882 - val_loss: nan - val_accuracy: 0.8553\n","Epoch 7/20\n","22/22 [==============================] - 2s 87ms/step - loss: nan - accuracy: 0.7882 - val_loss: nan - val_accuracy: 0.8553\n","Epoch 8/20\n","22/22 [==============================] - 2s 78ms/step - loss: nan - accuracy: 0.7882 - val_loss: nan - val_accuracy: 0.8553\n","Epoch 9/20\n","22/22 [==============================] - 2s 78ms/step - loss: nan - accuracy: 0.7882 - val_loss: nan - val_accuracy: 0.8553\n","Epoch 10/20\n","22/22 [==============================] - 2s 81ms/step - loss: nan - accuracy: 0.7882 - val_loss: nan - val_accuracy: 0.8553\n","Epoch 11/20\n","22/22 [==============================] - 2s 80ms/step - loss: nan - accuracy: 0.7882 - val_loss: nan - val_accuracy: 0.8553\n","Epoch 12/20\n","22/22 [==============================] - 2s 90ms/step - loss: nan - accuracy: 0.7882 - val_loss: nan - val_accuracy: 0.8553\n","Epoch 13/20\n","22/22 [==============================] - 3s 140ms/step - loss: nan - accuracy: 0.7882 - val_loss: nan - val_accuracy: 0.8553\n","Epoch 14/20\n","22/22 [==============================] - 3s 116ms/step - loss: nan - accuracy: 0.7882 - val_loss: nan - val_accuracy: 0.8553\n","Epoch 15/20\n","22/22 [==============================] - 2s 79ms/step - loss: nan - accuracy: 0.7882 - val_loss: nan - val_accuracy: 0.8553\n","Epoch 16/20\n","22/22 [==============================] - 2s 79ms/step - loss: nan - accuracy: 0.7882 - val_loss: nan - val_accuracy: 0.8553\n","Epoch 17/20\n","22/22 [==============================] - 2s 82ms/step - loss: nan - accuracy: 0.7882 - val_loss: nan - val_accuracy: 0.8553\n","Epoch 18/20\n","22/22 [==============================] - 2s 83ms/step - loss: nan - accuracy: 0.7882 - val_loss: nan - val_accuracy: 0.8553\n","Epoch 19/20\n","22/22 [==============================] - 2s 79ms/step - loss: nan - accuracy: 0.7882 - val_loss: nan - val_accuracy: 0.8553\n","Epoch 20/20\n","22/22 [==============================] - 3s 120ms/step - loss: nan - accuracy: 0.7882 - val_loss: nan - val_accuracy: 0.8553\n","6/6 [==============================] - 0s 36ms/step - loss: nan - accuracy: 0.7672\n","Test Accuracy: 76.72%\n","6/6 [==============================] - 0s 33ms/step\n","[[145   0]\n"," [ 44   0]]\n","              precision    recall  f1-score   support\n","\n","           0       0.77      1.00      0.87       145\n","           1       0.00      0.00      0.00        44\n","\n","    accuracy                           0.77       189\n","   macro avg       0.38      0.50      0.43       189\n","weighted avg       0.59      0.77      0.67       189\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler, OneHotEncoder\n","from sklearn.compose import ColumnTransformer\n","from sklearn.pipeline import Pipeline\n","from sklearn.metrics import classification_report, confusion_matrix\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n","from google.colab import drive\n","\n","drive.mount(\"/content/drive\")\n","\n","# Load the dataset\n","data = pd.read_csv(\"/content/drive/MyDrive/Thesis/EEG.machinelearing_data_BRMH.csv\")\n","\n","# Assuming 'Addictive disorder' corresponds to depression\n","# Replace 'Addictive disorder' with the actual value representing depression in your dataset\n","y = data['specific.disorder'].apply(lambda x: 1 if x == 'Depressive disorder' else 0).values  # Convert to NumPy array\n","\n","# Drop non-numeric and unnecessary columns for X (features)\n","X = data.drop(['no.', 'sex', 'eeg.date', 'education', 'IQ', 'specific.disorder'], axis=1)\n","\n","# Identify numeric and categorical columns\n","numeric_cols = X.select_dtypes(include=np.number).columns\n","categorical_cols = list(set(X.columns) - set(numeric_cols))\n","\n","# Create transformers for numeric and categorical columns\n","numeric_transformer = StandardScaler()\n","categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n","\n","# Create a preprocessor to handle both types of columns\n","preprocessor = ColumnTransformer(\n","    transformers=[\n","        ('num', numeric_transformer, numeric_cols),\n","        ('cat', categorical_transformer, categorical_cols)\n","    ])\n","\n","# Create a pipeline for preprocessing and reshaping\n","pipeline = Pipeline(steps=[('preprocessor', preprocessor)])\n","\n","# Use the same preprocessing pipeline for both X_train and X_test\n","X_preprocessed = pipeline.fit_transform(X)\n","X_reshaped = X_preprocessed[:, :, np.newaxis].astype(np.float32)\n","\n","# Split the preprocessed data\n","X_train, X_test, y_train, y_test = train_test_split(X_reshaped, y, test_size=0.2, random_state=42)\n","\n","# Rebuild and retrain the model\n","model = Sequential()\n","model.add(Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)))\n","model.add(MaxPooling1D(pool_size=2))\n","model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n","model.add(MaxPooling1D(pool_size=2))\n","model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n","model.add(MaxPooling1D(pool_size=2))\n","model.add(Flatten())\n","model.add(Dense(64, activation='relu'))\n","model.add(Dense(1, activation='sigmoid'))\n","model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","model.summary()\n","\n","history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.1)  # Increase epochs\n","\n","# Evaluate the Model\n","loss, accuracy = model.evaluate(X_test, y_test)\n","print(f'Test Accuracy: {accuracy * 100:.2f}%')\n","\n","# Generate predictions and evaluate performance\n","y_pred_prob = model.predict(X_test)\n","y_pred = (y_pred_prob > 0.5).astype(int)\n","\n","print(confusion_matrix(y_test, y_pred))\n","print(classification_report(y_test, y_pred))\n"]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNzJEmBF0XCImojXlQsjRL8"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}